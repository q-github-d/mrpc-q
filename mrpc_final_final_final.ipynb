{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOw_jALtfVhE",
        "outputId": "fe68f3c4-97e2-4ba2-8576-a7a780ef8ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import csv, json\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the input data from here \n",
        "# https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\n",
        "# Download it using \n",
        "\n",
        "# python untitled.py --data_dir glue_data --tasks all\n",
        "\n",
        "#Add the following lines in the  untitled.py to properly download the data\n",
        "\n",
        "# import io\n",
        "# URLLIB = urllib.request\n",
        "# 'MRPC':'https://raw.githubusercontent.com/MegEngine/Models/master/official/nlp/bert/glue_data/MRPC/dev_ids.tsv' inside TASK2PATH \n",
        "\n",
        "# !python untitled.py --data_dir glue_data --tasks all"
      ],
      "metadata": {
        "id": "CdEjWYsDgLxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42a201e0-cd0b-4b3b-de14-7077c80d3f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and extracting CoLA...\n",
            "\tCompleted!\n",
            "Downloading and extracting SST...\n",
            "\tCompleted!\n",
            "Processing MRPC...\n",
            "\tCompleted!\n",
            "Downloading and extracting QQP...\n",
            "\tCompleted!\n",
            "Downloading and extracting STS...\n",
            "\tCompleted!\n",
            "Downloading and extracting MNLI...\n",
            "\tNote (12/10/20): This script no longer downloads SNLI. You will need to manually download and format the data to use SNLI.\n",
            "\tCompleted!\n",
            "Downloading and extracting QNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting RTE...\n",
            "\tCompleted!\n",
            "Downloading and extracting WNLI...\n",
            "\tCompleted!\n",
            "Downloading and extracting diagnostic...\n",
            "\tCompleted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION_PAIRS_FILE = '/content/glue_data/MRPC/msr_paraphrase_train.txt'\n",
        "QUESTION_PAIRS_FILE_TEST = '/content/glue_data/MRPC/msr_paraphrase_test.txt'\n",
        "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.840B.300d.zip'\n",
        "GLOVE_ZIP_FILE = 'glove.840B.300d.zip'\n",
        "GLOVE_FILE = 'glove.840B.300d.txt'\n",
        "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
        "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
        "\n",
        "Q1_TESTING_DATA_FILE = 'q1_test.npy'\n",
        "Q2_TESTING_DATA_FILE = 'q2_test.npy'\n",
        "LABEL_TESTING_DATA_FILE = 'label_test.npy'\n",
        "\n",
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 300"
      ],
      "metadata": {
        "id": "0sm3xnPVfaIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list(filename_path):\n",
        "  \"\"\"Read the data from file and return the relevant data in list\n",
        "\n",
        "  Parameters:\n",
        "  filename_path (str): Location of File\n",
        "\n",
        "  Returns:\n",
        "  list: sentence1\n",
        "  list: sentence2\n",
        "  list: label\n",
        "  \n",
        "  \"\"\"\n",
        "  print(\"Processing\", filename_path)\n",
        "\n",
        "  sentence1 = []\n",
        "  sentence2 = []\n",
        "  is_duplicate = []\n",
        "  with open(filename_path, encoding='utf-8') as csvfile:\n",
        "      fieldnames = ['label', 'id1', 'id2', 'sentence1', 'sentence2']\n",
        "      reader  = pd.read_csv(filename_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "      reader.columns = fieldnames\n",
        "      totalrows = 0\n",
        "      for ind, row in reader.iterrows():\n",
        "        totalrows+=1\n",
        "        if row['sentence1'] is None:\n",
        "          continue\n",
        "        if row['sentence2'] is None:\n",
        "          continue\n",
        "        if row['label'] is None: \n",
        "          continue\n",
        "        sentence1.append(row['sentence1'])\n",
        "        sentence2.append(row['sentence2'])\n",
        "        is_duplicate.append(row['label'])\n",
        "      print(totalrows)\n",
        "\n",
        "  print('Question pairs: %d' % len(sentence1))\n",
        "  return sentence1, sentence2, is_duplicate"
      ],
      "metadata": {
        "id": "EaWAEyv8gBXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1, sentence2, is_duplicate= get_list(QUESTION_PAIRS_FILE)\n",
        "sentence1_test, sentence2_test, is_duplicate_test= get_list(QUESTION_PAIRS_FILE_TEST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98U4Dti97KaO",
        "outputId": "efe80e61-4c7e-4238-d906-884a82242cff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/glue_data/MRPC/msr_paraphrase_train.txt\n",
            "4076\n",
            "Question pairs: 4076\n",
            "Processing /content/glue_data/MRPC/msr_paraphrase_test.txt\n",
            "1725\n",
            "Question pairs: 1725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(is_duplicate_test)"
      ],
      "metadata": {
        "id": "XHIY0CyoXMTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e521ab83-807b-448b-c882-70c857b177b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1725"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(list_of_test):\n",
        "  \"\"\"Preprocessing on input data. Removing stopwords, punctuation and making lowercase.\n",
        "\n",
        "  Parameters:\n",
        "  list_of_test (list): list of sentences \n",
        "\n",
        "  Returns:\n",
        "  list: Processed List\n",
        "  \n",
        "  \"\"\"\n",
        "  final_list = []\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  for x in list_of_test:\n",
        "    punctuationfree=\"\".join([i.lower() for i in x if i not in string.punctuation])\n",
        "    filtered = ' '.join([word for word in punctuationfree.split() if word not in stopwords])\n",
        "    final_list.append(filtered)\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "945mUkasv8EC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = remove_punctuation(sentence1)\n",
        "sentence2 = remove_punctuation(sentence2)\n",
        "sentence1_test = remove_punctuation(sentence1_test)\n",
        "sentence2_test = remove_punctuation(sentence2_test)"
      ],
      "metadata": {
        "id": "1nwxldJHw8Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_tokenizer(sentence1, sentence2):\n",
        "  \"\"\"Tokenizes the input data\n",
        "\n",
        "  Parameters:\n",
        "  sentence1 (list): Sentence1\n",
        "  sentence2 (list): Sentence2\n",
        "\n",
        "  Returns:\n",
        "  dict: word_index\n",
        "  list: sentence1 word sequence\n",
        "  list: sentence2 word sequence\n",
        "  tokenizer: tokenizer fitted on train data\n",
        "  \n",
        "  \"\"\"\n",
        "  questions = sentence1 + sentence2\n",
        "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(questions)\n",
        "  sentence1_word_sequences = tokenizer.texts_to_sequences(sentence1)\n",
        "  sentence2_word_sequences = tokenizer.texts_to_sequences(sentence2)\n",
        "  word_index = tokenizer.word_index\n",
        "\n",
        "  print(\"Words in index: %d\" % len(word_index))\n",
        "  return word_index, sentence1_word_sequences, sentence2_word_sequences, tokenizer"
      ],
      "metadata": {
        "id": "WCKt-60qiAk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "work_index, sentence1_word_sequences, sentence2_word_sequences, tokenizer = my_tokenizer(sentence1, sentence2)\n",
        "sentence1_word_sequences_test = tokenizer.texts_to_sequences(sentence1_test)\n",
        "sentence2_word_sequences_test = tokenizer.texts_to_sequences(sentence2_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfwjIdhC8CDE",
        "outputId": "103e3e0b-ac11-40d0-eece-912c9fc965a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in index: 14246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the GLove file and extracting embeddings_index\n",
        "\n",
        "if not exists(GLOVE_ZIP_FILE):\n",
        "    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
        "    zipfile.extract(GLOVE_FILE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwTgZFr2iGB0",
        "outputId": "5f3419b1-4c4a-4d3f-bf09-2940085eba22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://nlp.stanford.edu/data/glove.840B.300d.zip\n",
            "2176768927/2176768927 [==============================] - 410s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing\", GLOVE_FILE)\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(GLOVE_FILE, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings: %d' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvyT1Ooakxrd",
        "outputId": "fa5ba754-6741-4eb0-d5e6-29775f323440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing glove.840B.300d.txt\n",
            "Word embeddings: 2196016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(word_index):\n",
        "  \"\"\"Get embeddings matriz for the words in word_index\n",
        "\n",
        "  Parameters:\n",
        "  word_index (dict): word_index from tokenizer\n",
        "\n",
        "  Returns:\n",
        "  list: word_embedding_matrix\n",
        "  int: number of words\n",
        "  \n",
        "  \"\"\"  \n",
        "  nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "  word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      if i > MAX_NB_WORDS:\n",
        "          continue\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          word_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
        "  return word_embedding_matrix, nb_words"
      ],
      "metadata": {
        "id": "-cTQEAy9m5i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding_matrix, nb_words = get_embeddings(work_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fidKntnT9x0c",
        "outputId": "1987a9b3-8fa0-422f-bc35-00312332b494"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null word embeddings: 1418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def finaL_save(sentence1_word_sequences, sentence2_word_sequences, is_duplicate):\n",
        "  \"\"\"Add padding to the tokeinzed data\n",
        "\n",
        "  Parameters:\n",
        "  list: sentence1 word sequence\n",
        "  list: sentence2 word sequence\n",
        "  list: lables\n",
        "\n",
        "  Returns:\n",
        "  numpy.ndarray: Equal sized word sequence for sentence 1\n",
        "  numpy.ndarray: Equal sized word sequence for sentence 1\n",
        "  numpy.ndarray: labels\n",
        "  \n",
        "  \"\"\"  \n",
        "  q1_data = pad_sequences(sentence1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  q2_data = pad_sequences(sentence2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  labels = np.array(is_duplicate, dtype=int)\n",
        "  print('Shape of sentence1 data tensor:', q1_data.shape)\n",
        "  print('Shape of sentence2 data tensor:', q2_data.shape)\n",
        "  print('Shape of label tensor:', labels.shape)\n",
        "  return q1_data, q2_data, labels"
      ],
      "metadata": {
        "id": "rxQ1rbiTnmuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_data, q2_data, labels= finaL_save(sentence1_word_sequences, sentence2_word_sequences, is_duplicate)\n",
        "q1_data_test, q2_data_test, labels_test= finaL_save(sentence1_word_sequences_test, sentence2_word_sequences_test, is_duplicate_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOCHSFwj-UM0",
        "outputId": "bfc9f822-19ad-497b-be6d-1637a6421f10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of sentence1 data tensor: (4076, 25)\n",
            "Shape of sentence2 data tensor: (4076, 25)\n",
            "Shape of label tensor: (4076,)\n",
            "Shape of sentence1 data tensor: (1725, 25)\n",
            "Shape of sentence2 data tensor: (1725, 25)\n",
            "Shape of label tensor: (1725,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), q1_data)\n",
        "np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), q2_data)\n",
        "np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
        "np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
        "with open(NB_WORDS_DATA_FILE, 'w') as f:\n",
        "    json.dump({'nb_words': nb_words}, f)\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "UdV3R-g1npeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(open(Q1_TESTING_DATA_FILE, 'wb'), q1_data_test)\n",
        "np.save(open(Q2_TESTING_DATA_FILE, 'wb'), q2_data_test)\n",
        "np.save(open(LABEL_TESTING_DATA_FILE, 'wb'), labels_test)"
      ],
      "metadata": {
        "id": "DF7WZIJ3_Krh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, time, json\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Bidirectional, LSTM, dot, Flatten, Dense, Reshape, add, Dropout, BatchNormalization,TimeDistributed, Lambda, concatenate\n",
        "from keras.layers import Embedding\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "o0dLQ_Ccny_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
        "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
        "\n",
        "Q1_TESTING_DATA_FILE = 'q1_test.npy'\n",
        "Q2_TESTING_DATA_FILE = 'q2_test.npy'\n",
        "LABEL_TESTING_DATA_FILE = 'label_test.npy'\n",
        "\n",
        "MODEL_WEIGHTS_FILE = 'model_weights.h5'\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "WORD_EMBEDDING_DIM = 300\n",
        "SENT_EMBEDDING_DIM = 128\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 13371447\n",
        "NB_EPOCHS = 25\n",
        "DROPOUT = 0.2\n",
        "BATCH_SIZE = 516"
      ],
      "metadata": {
        "id": "2vRCFnRXn7D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
        "q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
        "labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
        "word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
        "with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
        "    nb_words = json.load(f)['nb_words']\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer2 = pickle.load(handle)"
      ],
      "metadata": {
        "id": "6Ic13we4oLDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOhcyCv6oNgz",
        "outputId": "37a30c99-4aab-4107-a4de-c4440aa90aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4076, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.stack((q1_data, q2_data), axis=1)\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
        "Q1_train = X_train[:,0]\n",
        "Q2_train = X_train[:,1]\n",
        "Q1_test = X_test[:,0]\n",
        "Q2_test = X_test[:,1]"
      ],
      "metadata": {
        "id": "no1qJFZOoVaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "sentence2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "q1 = Embedding(nb_words + 1, \n",
        "                 WORD_EMBEDDING_DIM, \n",
        "                 weights=[word_embedding_matrix], \n",
        "                 input_length=MAX_SEQUENCE_LENGTH, \n",
        "                 trainable=False)(sentence1)\n",
        "q1 = Bidirectional(LSTM(SENT_EMBEDDING_DIM, return_sequences=True), merge_mode=\"sum\")(q1)\n",
        "\n",
        "q2 = Embedding(nb_words + 1, \n",
        "                 WORD_EMBEDDING_DIM, \n",
        "                 weights=[word_embedding_matrix], \n",
        "                 input_length=MAX_SEQUENCE_LENGTH, \n",
        "                 trainable=False)(sentence2)\n",
        "q2 = Bidirectional(LSTM(SENT_EMBEDDING_DIM, return_sequences=True), merge_mode=\"sum\")(q2)\n",
        "\n",
        "attention = dot([q1,q2], [1,1])\n",
        "attention = Flatten()(attention)\n",
        "attention = Dense((MAX_SEQUENCE_LENGTH*SENT_EMBEDDING_DIM))(attention)\n",
        "attention = Reshape((MAX_SEQUENCE_LENGTH, SENT_EMBEDDING_DIM))(attention)\n",
        "\n",
        "merged = add([q1,attention])\n",
        "merged = Flatten()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "# merged = Dense(200, activation='relu')(merged)\n",
        "# merged = Dropout(DROPOUT)(merged)\n",
        "# merged = BatchNormalization()(merged)\n",
        "# merged = Dense(200, activation='relu')(merged)\n",
        "# merged = Dropout(DROPOUT)(merged)\n",
        "# merged = BatchNormalization()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "model = Model(inputs=[sentence1,sentence2], outputs=is_duplicate)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "h88IukHls4jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHeno9ZImKSA",
        "outputId": "f9009688-0530-4640-93bd-4a6a522cdb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 25, 300)      4274100     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)        (None, 25, 300)      4274100     ['input_2[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional (Bidirectional)  (None, 25, 128)      439296      ['embedding[0][0]']              \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirectional  (None, 25, 128)     439296      ['embedding_1[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dot (Dot)                      (None, 128, 128)     0           ['bidirectional[0][0]',          \n",
            "                                                                  'bidirectional_1[0][0]']        \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 16384)        0           ['dot[0][0]']                    \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 3200)         52432000    ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " reshape (Reshape)              (None, 25, 128)      0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 25, 128)      0           ['bidirectional[0][0]',          \n",
            "                                                                  'reshape[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_1 (Flatten)            (None, 3200)         0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 200)          640200      ['flatten_1[0][0]']              \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 200)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 200)         800         ['dropout[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 200)          40200       ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 200)          0           ['dense_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 200)         800         ['dropout_1[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_3 (Dense)                (None, 1)            201         ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 62,540,993\n",
            "Trainable params: 53,991,993\n",
            "Non-trainable params: 8,549,000\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training at\", datetime.datetime.now())\n",
        "t0 = time.time()\n",
        "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_accuracy', save_best_only=True), EarlyStopping(monitor='val_loss', mode='min', verbose=1)]\n",
        "history = model.fit([Q1_train, Q2_train],\n",
        "                    y_train,\n",
        "                    epochs=NB_EPOCHS,\n",
        "                    validation_split=VALIDATION_SPLIT,\n",
        "                    verbose=2,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=callbacks)\n",
        "t1 = time.time()\n",
        "print(\"Training ended at\", datetime.datetime.now())\n",
        "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63TdsBLYs6zJ",
        "outputId": "9cddb293-9b31-487d-8c53-8edd9aa29882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training at 2023-03-24 11:27:32.356029\n",
            "Epoch 1/25\n",
            "7/7 - 39s - loss: 0.8179 - accuracy: 0.5659 - val_loss: 1.2923 - val_accuracy: 0.4932 - 39s/epoch - 6s/step\n",
            "Epoch 2/25\n",
            "7/7 - 29s - loss: 0.6666 - accuracy: 0.6183 - val_loss: 0.8688 - val_accuracy: 0.6785 - 29s/epoch - 4s/step\n",
            "Epoch 2: early stopping\n",
            "Training ended at 2023-03-24 11:28:39.988074\n",
            "Minutes elapsed: 1.127197\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
        "                    'training': history.history['accuracy'],\n",
        "                    'validation': history.history['val_accuracy']})\n",
        "ax = acc.set_index('epoch').plot()\n",
        "# ax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_ylim([0.0,1.0]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ZARry3GqtC43",
        "outputId": "81739e79-7865-41aa-97cf-8bf390367007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_accuracy']))\n",
        "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfjJW03sxQbC",
        "outputId": "a54b8de5-e38e-48f8-b943-9fba146e79b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum accuracy at epoch 2 = 0.6785\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_weights(MODEL_WEIGHTS_FILE)\n",
        "loss, accuracy = model.evaluate([q1_data_test, q2_data_test], np.asarray(is_duplicate_test), verbose=0)\n",
        "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))\n",
        "# loss = 0.7759, accuracy = 0.6858"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAGYRhXeyHUT",
        "outputId": "dfbb8645-3fac-4685-bf04-f10f4c72b902"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 0.9138, accuracy = 0.6841\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(MODEL_WEIGHTS_FILE)"
      ],
      "metadata": {
        "id": "7N3A8kayyMxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_weights(MODEL_WEIGHTS_FILE)"
      ],
      "metadata": {
        "id": "QH-XIQfkhzsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "\n",
        "y_pred_actuals = model.predict([q1_data_test, q2_data_test])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T5YoQEler-S",
        "outputId": "40712094-f035-4726-ccd5-a4c18f640aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54/54 [==============================] - 9s 108ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_THRESHOLD = 0.8\n",
        "y_pred =(y_pred_actuals>MODEL_THRESHOLD)\n",
        "list(y_pred)\n",
        "\n",
        "cm = confusion_matrix(is_duplicate_test, y_pred)\n",
        "print(cm)\n",
        "print(precision_score(is_duplicate_test, y_pred , average=\"macro\"))\n",
        "print(recall_score(is_duplicate_test, y_pred , average=\"macro\"))\n",
        "print(\"F1 Score\", f1_score(is_duplicate_test, y_pred , average=\"macro\"))\n",
        "print(\"Accuracy\", accuracy_score(is_duplicate_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU3U6hZ-yYl2",
        "outputId": "aaa91f0f-8170-4f78-eedb-79051ea8e272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[210 368]\n",
            " [274 873]]\n",
            "0.5686746225717729\n",
            "0.5622188769861501\n",
            "F1 Score 0.5633180024415865\n",
            "Accuracy 0.6278260869565218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(Counter(list(y_train)))\n",
        "print(Counter(list(is_duplicate_test)))\n",
        "print(Counter(list(y_pred.flatten())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep7m8sM7F-wl",
        "outputId": "61c349b5-0848-4fd1-d7eb-f85faade8d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({1: 2467, 0: 1201})\n",
            "Counter({1: 1147, 0: 578})\n",
            "Counter({True: 1241, False: 484})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip list"
      ],
      "metadata": {
        "id": "-gFZr6guHv8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld7IsGHSY7Y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}