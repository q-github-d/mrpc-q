{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOw_jALtfVhE",
        "outputId": "82950b74-af73-4376-df4c-652bdc7a79cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import csv, json\n",
        "from zipfile import ZipFile\n",
        "from os.path import expanduser, exists\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils.data_utils import get_file\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVJowx_s_ix0",
        "outputId": "4e33adb4-59a7-4fd4-9c73-2b905cf9b9a1"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Download the input data from here \n",
        "# https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e\n",
        "# Download it using \n",
        "\n",
        "# python untitled.py --data_dir glue_data --tasks all\n",
        "\n",
        "#Add the following lines in the  untitled.py to properly download the data\n",
        "\n",
        "# import io\n",
        "# URLLIB = urllib.request\n",
        "# 'MRPC':'https://raw.githubusercontent.com/MegEngine/Models/master/official/nlp/bert/glue_data/MRPC/dev_ids.tsv' inside TASK2PATH \n",
        "\n",
        "# !python untitled.py --data_dir glue_data --tasks all"
      ],
      "metadata": {
        "id": "CdEjWYsDgLxs"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUESTION_PAIRS_FILE = '/content/glue_data/MRPC/msr_paraphrase_train.txt'\n",
        "QUESTION_PAIRS_FILE_TEST = '/content/glue_data/MRPC/msr_paraphrase_test.txt'\n",
        "GLOVE_ZIP_FILE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "GLOVE_ZIP_FILE = 'glove.6B.zip'\n",
        "GLOVE_FILE = 'glove.6B.100d.txt'\n",
        "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
        "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
        "\n",
        "Q1_TESTING_DATA_FILE = 'q1_test.npy'\n",
        "Q2_TESTING_DATA_FILE = 'q2_test.npy'\n",
        "LABEL_TESTING_DATA_FILE = 'label_test.npy'\n",
        "\n",
        "MAX_NB_WORDS = 200000\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "EMBEDDING_DIM = 100"
      ],
      "metadata": {
        "id": "0sm3xnPVfaIn"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_list(filename_path):\n",
        "  \"\"\"Read the data from file and return the relevant data in list\n",
        "\n",
        "  Parameters:\n",
        "  filename_path (str): Location of File\n",
        "\n",
        "  Returns:\n",
        "  list: sentence1\n",
        "  list: sentence2\n",
        "  list: label\n",
        "  \n",
        "  \"\"\"\n",
        "  print(\"Processing\", filename_path)\n",
        "\n",
        "  sentence1 = []\n",
        "  sentence2 = []\n",
        "  is_duplicate = []\n",
        "  with open(filename_path, encoding='utf-8') as csvfile:\n",
        "      fieldnames = ['label', 'id1', 'id2', 'sentence1', 'sentence2']\n",
        "      reader  = pd.read_csv(filename_path, sep='\\t', quoting=csv.QUOTE_NONE)\n",
        "      reader.columns = fieldnames\n",
        "      totalrows = 0\n",
        "      for ind, row in reader.iterrows():\n",
        "        totalrows+=1\n",
        "        if row['sentence1'] is None:\n",
        "          continue\n",
        "        if row['sentence2'] is None:\n",
        "          continue\n",
        "        if row['label'] is None: \n",
        "          continue\n",
        "        sentence1.append(row['sentence1'])\n",
        "        sentence2.append(row['sentence2'])\n",
        "        is_duplicate.append(row['label'])\n",
        "      print(totalrows)\n",
        "\n",
        "  print('Question pairs: %d' % len(sentence1))\n",
        "  return sentence1, sentence2, is_duplicate"
      ],
      "metadata": {
        "id": "EaWAEyv8gBXs"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1, sentence2, is_duplicate= get_list(QUESTION_PAIRS_FILE)\n",
        "sentence1_test, sentence2_test, is_duplicate_test= get_list(QUESTION_PAIRS_FILE_TEST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98U4Dti97KaO",
        "outputId": "2ddeafb5-aeb7-47a7-defd-f910139e8313"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/glue_data/MRPC/msr_paraphrase_train.txt\n",
            "4076\n",
            "Question pairs: 4076\n",
            "Processing /content/glue_data/MRPC/msr_paraphrase_test.txt\n",
            "1725\n",
            "Question pairs: 1725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(is_duplicate_test)"
      ],
      "metadata": {
        "id": "XHIY0CyoXMTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1284c817-e3de-4087-c45f-14e25f1b2406"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1725"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(list_of_test):\n",
        "  \"\"\"Preprocessing on input data. Removing stopwords, punctuation and making lowercase.\n",
        "\n",
        "  Parameters:\n",
        "  list_of_test (list): list of sentences \n",
        "\n",
        "  Returns:\n",
        "  list: Processed List\n",
        "  \n",
        "  \"\"\"\n",
        "  final_list = []\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  for x in list_of_test:\n",
        "    punctuationfree=\"\".join([i.lower() for i in x if i not in string.punctuation])\n",
        "    filtered = ' '.join([word for word in punctuationfree.split() if word not in stopwords])\n",
        "    final_list.append(filtered)\n",
        "  return final_list"
      ],
      "metadata": {
        "id": "945mUkasv8EC"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = remove_punctuation(sentence1)\n",
        "sentence2 = remove_punctuation(sentence2)\n",
        "sentence1_test = remove_punctuation(sentence1_test)\n",
        "sentence2_test = remove_punctuation(sentence2_test)"
      ],
      "metadata": {
        "id": "1nwxldJHw8Hn"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def my_tokenizer(sentence1, sentence2):\n",
        "  \"\"\"Tokenizes the input data\n",
        "\n",
        "  Parameters:\n",
        "  sentence1 (list): Sentence1\n",
        "  sentence2 (list): Sentence2\n",
        "\n",
        "  Returns:\n",
        "  dict: word_index\n",
        "  list: sentence1 word sequence\n",
        "  list: sentence2 word sequence\n",
        "  tokenizer: tokenizer fitted on train data\n",
        "  \n",
        "  \"\"\"\n",
        "  questions = sentence1 + sentence2\n",
        "  tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
        "  tokenizer.fit_on_texts(questions)\n",
        "  sentence1_word_sequences = tokenizer.texts_to_sequences(sentence1)\n",
        "  sentence2_word_sequences = tokenizer.texts_to_sequences(sentence2)\n",
        "  word_index = tokenizer.word_index\n",
        "\n",
        "  print(\"Words in index: %d\" % len(word_index))\n",
        "  return word_index, sentence1_word_sequences, sentence2_word_sequences, tokenizer"
      ],
      "metadata": {
        "id": "WCKt-60qiAk8"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "work_index, sentence1_word_sequences, sentence2_word_sequences, tokenizer = my_tokenizer(sentence1, sentence2)\n",
        "sentence1_word_sequences_test = tokenizer.texts_to_sequences(sentence1_test)\n",
        "sentence2_word_sequences_test = tokenizer.texts_to_sequences(sentence2_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfwjIdhC8CDE",
        "outputId": "1493a18d-29fa-403d-d788-6d2127dd72ab"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words in index: 14246\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the GLove file and extracting embeddings_index\n",
        "\n",
        "if not exists(GLOVE_ZIP_FILE):\n",
        "    zipfile = ZipFile(get_file(GLOVE_ZIP_FILE, GLOVE_ZIP_FILE_URL))\n",
        "    zipfile.extract(GLOVE_FILE)"
      ],
      "metadata": {
        "id": "nwTgZFr2iGB0"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing\", GLOVE_FILE)\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(GLOVE_FILE, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split(' ')\n",
        "        word = values[0]\n",
        "        embedding = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = embedding\n",
        "\n",
        "print('Word embeddings: %d' % len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvyT1Ooakxrd",
        "outputId": "835c1a4f-ccce-44cc-aad7-e9dfde1b37bc"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing glove.6B.100d.txt\n",
            "Word embeddings: 400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(embeddings_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tu5Y3GM3oTg9",
        "outputId": "03f8ed63-a67c-4771-a6aa-2415bbf4aafd"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "400000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(word_index):\n",
        "  \"\"\"Get embeddings matriz for the words in word_index\n",
        "\n",
        "  Parameters:\n",
        "  word_index (dict): word_index from tokenizer\n",
        "\n",
        "  Returns:\n",
        "  list: word_embedding_matrix\n",
        "  int: number of words\n",
        "  \n",
        "  \"\"\"  \n",
        "  nb_words = min(MAX_NB_WORDS, len(word_index))\n",
        "  word_embedding_matrix = np.zeros((nb_words + 1, EMBEDDING_DIM))\n",
        "  for word, i in word_index.items():\n",
        "      if i > MAX_NB_WORDS:\n",
        "          continue\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "          word_embedding_matrix[i] = embedding_vector\n",
        "\n",
        "  print('Null word embeddings: %d' % np.sum(np.sum(word_embedding_matrix, axis=1) == 0))\n",
        "  return word_embedding_matrix, nb_words"
      ],
      "metadata": {
        "id": "-cTQEAy9m5i5"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_embedding_matrix, nb_words = get_embeddings(work_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fidKntnT9x0c",
        "outputId": "1afd28da-0912-4e9a-f88a-62ab26e1f42b"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Null word embeddings: 1474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def finaL_save(sentence1_word_sequences, sentence2_word_sequences, is_duplicate):\n",
        "  \"\"\"Add padding to the tokeinzed data\n",
        "\n",
        "  Parameters:\n",
        "  list: sentence1 word sequence\n",
        "  list: sentence2 word sequence\n",
        "  list: lables\n",
        "\n",
        "  Returns:\n",
        "  numpy.ndarray: Equal sized word sequence for sentence 1\n",
        "  numpy.ndarray: Equal sized word sequence for sentence 1\n",
        "  numpy.ndarray: labels\n",
        "  \n",
        "  \"\"\"  \n",
        "  q1_data = pad_sequences(sentence1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  q2_data = pad_sequences(sentence2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "  labels = np.array(is_duplicate, dtype=int)\n",
        "  print('Shape of sentence1 data tensor:', q1_data.shape)\n",
        "  print('Shape of sentence2 data tensor:', q2_data.shape)\n",
        "  print('Shape of label tensor:', labels.shape)\n",
        "  return q1_data, q2_data, labels"
      ],
      "metadata": {
        "id": "rxQ1rbiTnmuT"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_data, q2_data, labels= finaL_save(sentence1_word_sequences, sentence2_word_sequences, is_duplicate)\n",
        "q1_data_test, q2_data_test, labels_test= finaL_save(sentence1_word_sequences_test, sentence2_word_sequences_test, is_duplicate_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOCHSFwj-UM0",
        "outputId": "203beb37-5ac1-4caf-b78c-270652874124"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of sentence1 data tensor: (4076, 25)\n",
            "Shape of sentence2 data tensor: (4076, 25)\n",
            "Shape of label tensor: (4076,)\n",
            "Shape of sentence1 data tensor: (1725, 25)\n",
            "Shape of sentence2 data tensor: (1725, 25)\n",
            "Shape of label tensor: (1725,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(open(Q1_TRAINING_DATA_FILE, 'wb'), q1_data)\n",
        "np.save(open(Q2_TRAINING_DATA_FILE, 'wb'), q2_data)\n",
        "np.save(open(LABEL_TRAINING_DATA_FILE, 'wb'), labels)\n",
        "np.save(open(WORD_EMBEDDING_MATRIX_FILE, 'wb'), word_embedding_matrix)\n",
        "with open(NB_WORDS_DATA_FILE, 'w') as f:\n",
        "    json.dump({'nb_words': nb_words}, f)\n",
        "with open('tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "UdV3R-g1npeM"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(open(Q1_TESTING_DATA_FILE, 'wb'), q1_data_test)\n",
        "np.save(open(Q2_TESTING_DATA_FILE, 'wb'), q2_data_test)\n",
        "np.save(open(LABEL_TESTING_DATA_FILE, 'wb'), labels_test)"
      ],
      "metadata": {
        "id": "DF7WZIJ3_Krh"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime, time, json\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Bidirectional, LSTM, dot, Flatten, Dense, Reshape, add, Dropout, BatchNormalization,TimeDistributed, Lambda, concatenate\n",
        "from keras.layers import Embedding\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "o0dLQ_Ccny_y"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q1_TRAINING_DATA_FILE = 'q1_train.npy'\n",
        "Q2_TRAINING_DATA_FILE = 'q2_train.npy'\n",
        "LABEL_TRAINING_DATA_FILE = 'label_train.npy'\n",
        "WORD_EMBEDDING_MATRIX_FILE = 'word_embedding_matrix.npy'\n",
        "NB_WORDS_DATA_FILE = 'nb_words.json'\n",
        "\n",
        "Q1_TESTING_DATA_FILE = 'q1_test.npy'\n",
        "Q2_TESTING_DATA_FILE = 'q2_test.npy'\n",
        "LABEL_TESTING_DATA_FILE = 'label_test.npy'\n",
        "\n",
        "MODEL_WEIGHTS_FILE = 'model_weights.h5'\n",
        "MAX_SEQUENCE_LENGTH = 25\n",
        "WORD_EMBEDDING_DIM = 100\n",
        "SENT_EMBEDDING_DIM = 128\n",
        "VALIDATION_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.1\n",
        "RNG_SEED = 13371447\n",
        "NB_EPOCHS = 25\n",
        "DROPOUT = 0.2\n",
        "BATCH_SIZE = 516"
      ],
      "metadata": {
        "id": "2vRCFnRXn7D_"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_data = np.load(open(Q1_TRAINING_DATA_FILE, 'rb'))\n",
        "q2_data = np.load(open(Q2_TRAINING_DATA_FILE, 'rb'))\n",
        "labels = np.load(open(LABEL_TRAINING_DATA_FILE, 'rb'))\n",
        "word_embedding_matrix = np.load(open(WORD_EMBEDDING_MATRIX_FILE, 'rb'))\n",
        "with open(NB_WORDS_DATA_FILE, 'r') as f:\n",
        "    nb_words = json.load(f)['nb_words']\n",
        "with open('tokenizer.pickle', 'rb') as handle:\n",
        "    tokenizer2 = pickle.load(handle)"
      ],
      "metadata": {
        "id": "6Ic13we4oLDF"
      },
      "execution_count": 156,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOhcyCv6oNgz",
        "outputId": "cb0e3724-4f7f-40b3-8af4-9713e0639f54"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4076, 25)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.stack((q1_data, q2_data), axis=1)\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SPLIT, random_state=RNG_SEED)\n",
        "Q1_train = X_train[:,0]\n",
        "Q2_train = X_train[:,1]\n",
        "Q1_test = X_test[:,0]\n",
        "Q2_test = X_test[:,1]"
      ],
      "metadata": {
        "id": "no1qJFZOoVaz"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_embedding_matrix.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxMgDf10oBt6",
        "outputId": "dc978145-a4d4-4369-d73f-b996beb47d21"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(14247, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "sentence2 = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "q1 = Embedding(nb_words + 1, \n",
        "                 WORD_EMBEDDING_DIM, \n",
        "                 weights=[word_embedding_matrix], \n",
        "                 input_length=MAX_SEQUENCE_LENGTH, \n",
        "                 trainable=False)(sentence1)\n",
        "q1 = Bidirectional(LSTM(SENT_EMBEDDING_DIM, return_sequences=True), merge_mode=\"sum\")(q1)\n",
        "\n",
        "q2 = Embedding(nb_words + 1, \n",
        "                 WORD_EMBEDDING_DIM, \n",
        "                 weights=[word_embedding_matrix], \n",
        "                 input_length=MAX_SEQUENCE_LENGTH, \n",
        "                 trainable=False)(sentence2)\n",
        "q2 = Bidirectional(LSTM(SENT_EMBEDDING_DIM, return_sequences=True), merge_mode=\"sum\")(q2)\n",
        "\n",
        "attention = dot([q1,q2], [1,1])\n",
        "attention = Flatten()(attention)\n",
        "attention = Dense((MAX_SEQUENCE_LENGTH*SENT_EMBEDDING_DIM))(attention)\n",
        "attention = Reshape((MAX_SEQUENCE_LENGTH, SENT_EMBEDDING_DIM))(attention)\n",
        "\n",
        "merged = add([q1,attention])\n",
        "merged = Flatten()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "# merged = Dense(200, activation='relu')(merged)\n",
        "# merged = Dropout(DROPOUT)(merged)\n",
        "# merged = BatchNormalization()(merged)\n",
        "# merged = Dense(200, activation='relu')(merged)\n",
        "# merged = Dropout(DROPOUT)(merged)\n",
        "# merged = BatchNormalization()(merged)\n",
        "merged = Dense(200, activation='relu')(merged)\n",
        "merged = Dropout(DROPOUT)(merged)\n",
        "merged = BatchNormalization()(merged)\n",
        "\n",
        "is_duplicate = Dense(1, activation='sigmoid')(merged)\n",
        "\n",
        "model = Model(inputs=[sentence1,sentence2], outputs=is_duplicate)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "h88IukHls4jc"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHeno9ZImKSA",
        "outputId": "466f9e98-c1ce-4684-ac18-f8efc6a34bef"
      },
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_7 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " input_8 (InputLayer)           [(None, 25)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_5 (Embedding)        (None, 25, 100)      1424700     ['input_7[0][0]']                \n",
            "                                                                                                  \n",
            " embedding_6 (Embedding)        (None, 25, 100)      1424700     ['input_8[0][0]']                \n",
            "                                                                                                  \n",
            " bidirectional_4 (Bidirectional  (None, 25, 128)     234496      ['embedding_5[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_5 (Bidirectional  (None, 25, 128)     234496      ['embedding_6[0][0]']            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " dot_2 (Dot)                    (None, 128, 128)     0           ['bidirectional_4[0][0]',        \n",
            "                                                                  'bidirectional_5[0][0]']        \n",
            "                                                                                                  \n",
            " flatten_4 (Flatten)            (None, 16384)        0           ['dot_2[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_8 (Dense)                (None, 3200)         52432000    ['flatten_4[0][0]']              \n",
            "                                                                                                  \n",
            " reshape_2 (Reshape)            (None, 25, 128)      0           ['dense_8[0][0]']                \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 25, 128)      0           ['bidirectional_4[0][0]',        \n",
            "                                                                  'reshape_2[0][0]']              \n",
            "                                                                                                  \n",
            " flatten_5 (Flatten)            (None, 3200)         0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_9 (Dense)                (None, 200)          640200      ['flatten_5[0][0]']              \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 200)          0           ['dense_9[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 200)         800         ['dropout_4[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_10 (Dense)               (None, 200)          40200       ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 200)          0           ['dense_10[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 200)         800         ['dropout_5[0][0]']              \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 1)            201         ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 56,432,593\n",
            "Trainable params: 53,582,393\n",
            "Non-trainable params: 2,850,200\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Starting training at\", datetime.datetime.now())\n",
        "t0 = time.time()\n",
        "callbacks = [ModelCheckpoint(MODEL_WEIGHTS_FILE, monitor='val_accuracy', save_best_only=True), EarlyStopping(monitor='val_loss', mode='min', verbose=1)]\n",
        "history = model.fit([Q1_train, Q2_train],\n",
        "                    y_train,\n",
        "                    epochs=NB_EPOCHS,\n",
        "                    validation_split=VALIDATION_SPLIT,\n",
        "                    verbose=2,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    callbacks=callbacks)\n",
        "t1 = time.time()\n",
        "print(\"Training ended at\", datetime.datetime.now())\n",
        "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63TdsBLYs6zJ",
        "outputId": "8c869ffb-53d7-4f1a-b11d-1bc8fa217420"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training at 2023-03-24 12:40:21.151072\n",
            "Epoch 1/25\n",
            "7/7 - 52s - loss: 0.8752 - accuracy: 0.5526 - val_loss: 0.8031 - val_accuracy: 0.6594 - 52s/epoch - 7s/step\n",
            "Epoch 2/25\n",
            "7/7 - 40s - loss: 0.7197 - accuracy: 0.5765 - val_loss: 0.9451 - val_accuracy: 0.6839 - 40s/epoch - 6s/step\n",
            "Epoch 2: early stopping\n",
            "Training ended at 2023-03-24 12:41:53.320623\n",
            "Minutes elapsed: 1.536129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ],\n",
        "                    'training': history.history['accuracy'],\n",
        "                    'validation': history.history['val_accuracy']})\n",
        "ax = acc.set_index('epoch').plot()\n",
        "# ax = acc.iloc[:,:].plot(x='epoch', figsize={5,8}, grid=True)\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_ylim([0.0,1.0]);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ZARry3GqtC43",
        "outputId": "549a8e99-be6d-479c-e927-66be900295b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_val_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_accuracy']))\n",
        "print('Maximum accuracy at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(max_val_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfjJW03sxQbC",
        "outputId": "77f5947e-a751-4c43-9127-99842a6f4583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum accuracy at epoch 2 = 0.6839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_weights(MODEL_WEIGHTS_FILE)\n",
        "loss, accuracy = model.evaluate([q1_data_test, q2_data_test], np.asarray(is_duplicate_test), verbose=0)\n",
        "print('loss = {0:.4f}, accuracy = {1:.4f}'.format(loss, accuracy))\n",
        "# loss = 0.7759, accuracy = 0.6858"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAGYRhXeyHUT",
        "outputId": "95a95ce4-4bab-4b06-c315-d83b41d3f716"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss = 0.9784, accuracy = 0.6870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model.save(MODEL_WEIGHTS_FILE)"
      ],
      "metadata": {
        "id": "7N3A8kayyMxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_weights('my_model_weights.h5')\n",
        "json_string = model.to_json()"
      ],
      "metadata": {
        "id": "urLcMzbdqrCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('model_json.json', 'w') as f:\n",
        "    f.write(json_string)"
      ],
      "metadata": {
        "id": "hYUfbMG6uqfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.load_weights(MODEL_WEIGHTS_FILE)"
      ],
      "metadata": {
        "id": "QH-XIQfkhzsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "\n",
        "y_pred_actuals = model.predict([q1_data_test, q2_data_test])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9T5YoQEler-S",
        "outputId": "2272ff48-751d-4752-8446-33b0b80bf70a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "54/54 [==============================] - 12s 189ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_THRESHOLD = 0.8\n",
        "y_pred =(y_pred_actuals>MODEL_THRESHOLD)\n",
        "list(y_pred)\n",
        "\n",
        "print(\"Confusion Matrix\")\n",
        "cm = confusion_matrix(is_duplicate_test, y_pred)\n",
        "print(cm)\n",
        "print(\"Precision\", precision_score(is_duplicate_test, y_pred , average=\"macro\"))\n",
        "print(\"Recall\", recall_score(is_duplicate_test, y_pred , average=\"macro\"))\n",
        "print(\"F1 Score\", f1_score(is_duplicate_test, y_pred , average=\"macro\"))\n",
        "print(\"Accuracy\", accuracy_score(is_duplicate_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OU3U6hZ-yYl2",
        "outputId": "92721b4b-c111-40df-e135-25f3ba139b8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix\n",
            "[[209 369]\n",
            " [237 910]]\n",
            "Precision 0.5900516098269037\n",
            "Recall 0.5774828573411004\n",
            "F1 Score 0.5792046127885409\n",
            "Accuracy 0.648695652173913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "print(\"Training Counts\\n\\t\", Counter(list(y_train)))\n",
        "print(\"Test Counts\\n\\t\", Counter(list(is_duplicate_test)))\n",
        "print(\"Predictions Counts\\n\\t\", Counter(list(y_pred.flatten())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep7m8sM7F-wl",
        "outputId": "7f133037-fafb-44fd-e6aa-5d635754b1a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Counts\n",
            "\t Counter({1: 2467, 0: 1201})\n",
            "Test Counts\n",
            "\t Counter({1: 1147, 0: 578})\n",
            "Predictions Counts\n",
            "\t Counter({True: 1279, False: 446})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip list"
      ],
      "metadata": {
        "id": "-gFZr6guHv8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ld7IsGHSY7Y1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}